{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e71295a1-57cb-47e5-89bb-74604710fd41",
   "metadata": {},
   "source": [
    "[Gymnasium](https://github.com/Farama-Foundation/Gymnasium) is the most common library for RL environments. It picked up and improved the abandoned [OpenAI Gym](https://github.com/openai/gym), it is compatible with a large number of other simulators (e.g., [ALE](https://github.com/Farama-Foundation/Arcade-Learning-Environment) for Atari games and [MuJoCo](https://github.com/google-deepmind/mujoco)), and we will use it for this course. \n",
    "\n",
    "The only things you need to know for this course are how to make environments and how to run actions to collect data. Let's look at the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72c7c522-c373-4edc-8644-89250fb5ddb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "\n",
    "env = gymnasium.make(\"Pendulum-v1\")\n",
    "s, info = env.reset(seed=42)\n",
    "a = env.action_space.sample()\n",
    "s_next, r, terminated, truncated, info = env.step(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc37b621-92e0-493e-945b-9d147883a884",
   "metadata": {},
   "source": [
    "`gymnasium.make(env_id)` makes an instance of the environment. In this case, we are making the [`Pendulum-v1` environment](https://gymnasium.farama.org/environments/classic_control/pendulum/), a simple toy problem with continuous states and actions. \n",
    "You can pass optional arguments to `make`, depending on the environment. The most important that usually all environments support are:\n",
    "- `render_mode`, to either render the environment to visualize it (`render_mode=\"human\"`) or to learn from pixels (`render_mode=\"rgb_array\"`).\n",
    "- `max_episode_steps`, to set the maximum number of steps per episodes. More on this later.\n",
    "\n",
    "Before doing anything else, we must always call `env.reset()`. This resets the state of the simulator, making it ready to run a new episode. The `seed` argument is optional, but it ensures reproducibility of your experiment as the environment may have some stochasticity (e.g., the initial state is usually random).  \n",
    "Setting the seed with `reset(seed)` sets every random generator of the environment, most notably `env.np_random`, `env.observation_space.np_random`, and `env.action_space.np_random`.  \n",
    "**Note!** You either want to set the seed at every reset or only at the first reset. In the former case, you must be sure to set it always to a different seed, or you'll keep running the same episode (e.g., the initial state will always be the same). More on this later.\n",
    "\n",
    "Two of the most important attributes of the environment are `env.observation_space` and `env.action_space`. \n",
    "- If a space is continuous, it will be an instance of `gymnasium.spaces.Box` and it will have the attributes `.low` and `.high` to denote its upper/lower bounds.\n",
    "- If a space is discrete, it will be an instance of `gymnasium.spaces.Discrete` and will the have the attribute `.n` to denote the finite number of states/actions in the space.\n",
    "\n",
    "Spaces also have the `sample()` function to draw a random state/action according to the random uniform distribution. \n",
    "\n",
    "For example, `Pendulum-v1` has continuous state and action spaces, while `Acrobot-v1` has discrete actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a9af21d-3f0b-476f-ae8c-38b9f4ba2adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pendulum\n",
      "[-1. -1. -8.] [1. 1. 8.]\n",
      "[-2.] [2.]\n",
      "[-0.24875921]\n",
      "\n",
      "Acrobot\n",
      "[ -1.        -1.        -1.        -1.       -12.566371 -28.274334] [ 1.        1.        1.        1.       12.566371 28.274334]\n",
      "3\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import gymnasium\n",
    "\n",
    "env = gymnasium.make(\"Pendulum-v1\")\n",
    "print(\"Pendulum\")\n",
    "print(env.observation_space.low, env.observation_space.high)\n",
    "print(env.action_space.low, env.action_space.high)\n",
    "print(env.action_space.sample())\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Acrobot\")\n",
    "env = gymnasium.make(\"Acrobot-v1\")\n",
    "print(env.observation_space.low, env.observation_space.high)\n",
    "print(env.action_space.n)\n",
    "print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a125f1-4f14-4b43-a8c3-100df103891d",
   "metadata": {},
   "source": [
    "**Note!** See that continuous actions/states are **always** `np.arrays`, i.e., they have shape `(n,)` where `n` is the dimensionality of the state/action.  \n",
    "On the contrary, discrete states/actions are **always** integers, i.e., they have no shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c89def-757f-4a6e-871e-fc314f8b422f",
   "metadata": {},
   "source": [
    "Let's continue with the most important environment method: `step(action)`. This performs an agent-environment interaction and changes the internal state of the simulator. In turns, it returns:\n",
    "- `s_next`, the next state,\n",
    "- `r`, the reward,\n",
    "- `terminated`, `True` if the agent has reached a terminal (absorbing) state,\n",
    "- `truncated`, `True` if the agent has performed `max_episode_steps` in the environment.\n",
    "\n",
    "`info` is a dictionary (also returned by `reset()`) with any additional information that could be useful to the agent, to log important metrics, or just to debug. Usually it is empty, but **it is always returned**.\n",
    "\n",
    "**Note!** The state returned by the simulator is actually an **observation** of the state, i.e., it may not have all information to solve the task, or it may have redundant information (e.g., pixel observations instead of just coordinates). This is why, often, you'll find `s` and `s_next` named `obs` and `obs_next`, respectively.\n",
    "\n",
    "**Note!** The flag `terminated` is extremely important when we use TD methods, because it tells us when we have to stop bootstrapping. The flag `truncated`, instead, is used to know when to reset the environment and to distringuish episodes (e.g., if we use Monte Carlo methods).  \n",
    "For example, if `terminated == True`, the TD error of the transition is just $V(s) - r$ rather than $V(s) - (r + \\gamma V(s_\\text{next}))$. And we do not care about `truncated == True` in this case!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c555894-0a8c-4938-978e-48daa0f1866b",
   "metadata": {},
   "source": [
    "You now know the basics to simulate environment interactions. Here is a simple loop that runs a random-policy agent for a few episodes and collects data in lists. At the end, we stack data into `np.array`.\n",
    "\n",
    "To ensure reproducibility, we set the episode seed based on a unique hashing function depending on two integers:\n",
    "- A fixed seed that usually denotes the experiment seed. For example, if we run this code 10 times, we'd set `seed = 1, 2, ..., 10`.\n",
    "- The episode counter.\n",
    "\n",
    "Below, we use [Cantor pairing](https://en.wikipedia.org/wiki/Pairing_function), but any unique hashing function is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9faa2e33-acee-44ed-83db-169b973c2bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,) 200 (2000, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium\n",
    "\n",
    "def cantor_pairing(x, y):\n",
    "    return int(0.5 * (x + y) * (x + y + 1) + y)\n",
    "    \n",
    "env = gymnasium.make(\"Pendulum-v1\")\n",
    "n_episodes = 10\n",
    "seed = 1\n",
    "\n",
    "data = dict()\n",
    "data[\"s\"] = []\n",
    "data[\"a\"] = []\n",
    "data[\"r\"] = []\n",
    "data[\"s_next\"] = []\n",
    "data[\"terminated\"] = []\n",
    "data[\"truncated\"] = []\n",
    "\n",
    "for ep in range(n_episodes):\n",
    "    episode_seed = cantor_pairing(ep, seed)\n",
    "    s, _ = env.reset(seed=episode_seed)\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        a = env.action_space.sample()\n",
    "        s_next, r, terminated, truncated, _ = env.step(a)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        data[\"s\"].append(s)\n",
    "        data[\"a\"].append(s)\n",
    "        data[\"r\"].append(r)\n",
    "        data[\"s_next\"].append(s_next)\n",
    "        data[\"terminated\"].append(terminated)\n",
    "        data[\"truncated\"].append(truncated)\n",
    "\n",
    "        s = s_next\n",
    "\n",
    "s = np.vstack(data[\"s\"])\n",
    "print(env.observation_space.shape, env._max_episode_steps, s.shape)  # 10 episodes of 200 steps each"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb70a49-81dd-4e25-9816-31c6a81f029e",
   "metadata": {},
   "source": [
    "**Note!** We must update the current state with `s = s_next` to tell the agent that the environment has changed. But we must do it **after** we store `s`!\n",
    "\n",
    "This loop can be used to collect data based on **episodes**, i.e., it's good for Monte Carlo methods. \n",
    "It is inefficient, though, because we use lists. This cannot really be fixed if we do not know the number of steps beforehand (`Pendulum-v1` is an infinite-horizon MDP with 200 steps limit, but many other MDPs are different). \n",
    "\n",
    "If we collect data based on **steps**, however, we can pre-allocate memory with `np.array`. The idea is to initialize zeros-array and fill them in as we collect data, using an index/counter to keep track of how many samples we collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f2ad839-62b6-4cb5-8bd6-5e05e31e2c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,) 200 (1876, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium\n",
    "\n",
    "def cantor_pairing(x, y):\n",
    "    return int(0.5 * (x + y) * (x + y + 1) + y)\n",
    "    \n",
    "env = gymnasium.make(\"Pendulum-v1\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "n_steps = 1876\n",
    "seed = 1\n",
    "\n",
    "data = dict()\n",
    "data[\"s\"] = np.zeros((n_steps, state_dim))\n",
    "data[\"a\"] = np.zeros((n_steps, action_dim))\n",
    "data[\"r\"] = np.zeros((n_steps,))\n",
    "data[\"s_next\"] = np.zeros((n_steps, state_dim))\n",
    "data[\"terminated\"] = np.zeros((n_steps,))\n",
    "data[\"truncated\"] = np.zeros((n_steps,))\n",
    "\n",
    "idx_data = 0\n",
    "for ep in range(n_episodes):\n",
    "    episode_seed = cantor_pairing(ep, seed)\n",
    "    s, _ = env.reset(seed=episode_seed)\n",
    "    done = False\n",
    "\n",
    "    while not done and idx_data < n_steps:\n",
    "        a = env.action_space.sample()\n",
    "        s_next, r, terminated, truncated, _ = env.step(a)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        data[\"s\"][idx_data] = s\n",
    "        data[\"a\"][idx_data] = a\n",
    "        data[\"r\"][idx_data] = r\n",
    "        data[\"s_next\"][idx_data] = s_next\n",
    "        data[\"terminated\"][idx_data] = terminated\n",
    "        data[\"truncated\"][idx_data] = truncated\n",
    "\n",
    "        idx_data += 1\n",
    "        s = s_next\n",
    "\n",
    "    if idx_data == n_steps:\n",
    "        break\n",
    "\n",
    "print(env.observation_space.shape, env._max_episode_steps, data[\"s\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419d7c3d-8701-4eb7-b519-8e1919dfcf70",
   "metadata": {},
   "source": [
    "**Note!** If spaces are discrete, we don't need to check their dimensionality, and we can just initialize the data as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d11dd90-8ab3-4382-9ff7-5cd30e7ad647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6,) 500 (1876, 6)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium\n",
    "\n",
    "def cantor_pairing(x, y):\n",
    "    return int(0.5 * (x + y) * (x + y + 1) + y)\n",
    "    \n",
    "env = gymnasium.make(\"Acrobot-v1\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "n_steps = 1876\n",
    "seed = 1\n",
    "\n",
    "data = dict()\n",
    "data[\"s\"] = np.zeros((n_steps, state_dim))\n",
    "data[\"a\"] = np.zeros((n_steps,))  # THIS IS JUST A 1D ARRAY!\n",
    "data[\"r\"] = np.zeros((n_steps,))\n",
    "data[\"s_next\"] = np.zeros((n_steps, state_dim))\n",
    "data[\"terminated\"] = np.zeros((n_steps,))\n",
    "data[\"truncated\"] = np.zeros((n_steps,))\n",
    "\n",
    "idx_data = 0\n",
    "for ep in range(n_episodes):\n",
    "    episode_seed = cantor_pairing(ep, seed)\n",
    "    s, _ = env.reset(seed=episode_seed)\n",
    "    done = False\n",
    "\n",
    "    while not done and idx_data < n_steps:\n",
    "        a = env.action_space.sample()\n",
    "        s_next, r, terminated, truncated, _ = env.step(a)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        data[\"s\"][idx_data] = s\n",
    "        data[\"a\"][idx_data] = a\n",
    "        data[\"r\"][idx_data] = r\n",
    "        data[\"s_next\"][idx_data] = s_next\n",
    "        data[\"terminated\"][idx_data] = terminated\n",
    "        data[\"truncated\"][idx_data] = truncated\n",
    "\n",
    "        idx_data += 1\n",
    "        s = s_next\n",
    "\n",
    "    if idx_data == n_steps:\n",
    "        break\n",
    "\n",
    "print(env.observation_space.shape, env._max_episode_steps, data[\"s\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce01506-f11f-49b6-8c64-a59a3051f00b",
   "metadata": {},
   "source": [
    "This covers the basic of Gymnasium and data collection for RL.  \n",
    "You can have separate functions for data collection, or have it interleaved with the training of the agent. An example of the latter would be the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc542b2d-73bd-479a-8aba-9c81ade4f36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generic_training(**kwargs):\n",
    "    max_steps = 10000  # max training steps\n",
    "    n_samples = 100  # number of samples to collect before an update\n",
    "    episode = 0  # use it to keep track of the episode and set the seed\n",
    "    tot_steps = 0  # to know when to stop training\n",
    "    idx_data = 0  # to know where to store data\n",
    "    seed = 1  # seed of the experiment\n",
    "\n",
    "    data = dict()\n",
    "    data[\"s\"] = np.zeros((n_samples, state_dim))\n",
    "    data[\"a\"] = np.zeros((n_samples, action_dim))\n",
    "    data[\"r\"] = np.zeros((n_samples))\n",
    "    data[\"s_next\"] = np.zeros((n_samples, state_dim))\n",
    "    data[\"terminated\"] = np.zeros((n_samples))\n",
    "    data[\"truncated\"] = np.zeros((n_samples))\n",
    "\n",
    "    while tot_steps < max_steps:\n",
    "        episode_seed = cantor_pairing(episode, seed)\n",
    "        s, _ = env.reset(seed=episode_seed)\n",
    "        done = False\n",
    "        episode += 1\n",
    "\n",
    "        while not done and tot_steps < max_steps:\n",
    "            a = env.action_space.sample()  # have your policy here\n",
    "            s_next, r, terminated, truncated, _ = env.step(a)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # store data\n",
    "            data[\"s\"][idx_data] = s\n",
    "            data[\"a\"][idx_data] = a\n",
    "            data[\"r\"][idx_data] = r\n",
    "            data[\"s_next\"][idx_data] = s_next\n",
    "            data[\"terminated\"][idx_data] = terminated\n",
    "            data[\"truncated\"][idx_data] = truncated\n",
    "            idx_data += 1\n",
    "\n",
    "            if idx_data == n_samples:\n",
    "                idx_data = 0  # reset index, so next time you will overwrite old data\n",
    "                pass # do your update\n",
    "\n",
    "            if tot_steps % log_frequency == 0:\n",
    "                pass # log whatever you want and/or print info\n",
    "\n",
    "            s = s_next\n",
    "            tot_steps += 1\n",
    "            pass # do whatever else is needed, eg, decay exploration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
